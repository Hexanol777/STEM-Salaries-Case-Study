{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iut](https://github.com/Hexanol777/STEM-Salaries-Case-Study/tree/main/Phase%201/stock_image/IUT200.png)\n",
    "<hr style=\"margin-bottom: 40px;\">\n",
    "\n",
    "\n",
    "# STEM Jobs Salaries\n",
    "\n",
    "## Data Aquisition, Cleaning and Wrangling\n",
    "\n",
    "#### No data is clean right away! The data cleaning phase is a crucial step in any data analysis project as it involves identifying and correcting errors or inconsistencies in the data. This phase typically involves removing irrelevant data, handling missing values, standardizing data formats, and removing duplicates. Proper data cleaning ensures that our analysis is accurate and reliable and helps us draw meaningful insights and conclusions from the data.\n",
    "\n",
    "[Link to the Data used in this Notebook](https://drive.google.com/file/d/1IhXv0qcq7YFfBxc0BQB1-z74wF40ZnZn/view?usp=share_link)\n",
    "\n",
    "<img src=\"https://github.com/Hexanol777/STEM-Salaries-Case-Study/tree/main/Phase%201/stock_image/Process1.png\"\n",
    "    style=\"width:600px; float: bottom; margin: 1 80px 80px 80px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
    "\n",
    "## Data Extraction - Importing Modules\n",
    "\n",
    " During the data extraction phase, we obtained the data directly from [kaggle.com](www.kaggle.com), which is a popular platform for accessing and sharing datasets. By using Kaggle, we were able to search for and download datasets that were relevant to our analysis which in this case is STEM Jobs Salaries, and we could be confident in the quality of the data provided, as the usability of it was rated 10 in the website. Overall, the data extraction phase was streamlined and efficient, thanks to the availability and accessibility of high-quality data on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n",
    "\n",
    "## Loading The Initial Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/STEMJobs.csv\n",
    "# Note: incase if you are running this line locally you will be met with the error below\n",
    "# as this notebook is meant to be executed in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(\n",
    "    'data/STEMJobs.csv',\n",
    "    parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n",
    "\n",
    "## Data First Look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "In the data cleaning phase, we removed certain race categories, such as `Race_Asian`, `Race_Black`, `Race_White`, `Race_Two_Or_More`, alongside the `educational attainment binaries` as they were not beneficial to our analysis. By streamlining the dataset, we get a step closer to an effective analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Race_Asian', 'Race_Black', 'Race_White', 'Race_Two_Or_More', 'Race_Hispanic','Race'\n",
    "                , 'Masters_Degree', 'Bachelors_Degree', 'Doctorate_Degree', 'Highschool', 'Some_College'\n",
    "               , 'tag', 'rowNumber', 'otherdetails', 'dmaid', 'cityid']\n",
    "\n",
    "Data = Data.drop(columns=cols_to_drop)\n",
    "Data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n",
    "\n",
    "## Column Wrangling\n",
    "\n",
    "The location in the \"City, State\" format. which is not relevant to our analysis, as we are interested in country-level insights. In order to improve our analysis, we need to homogenize the location data by creating a new column called 'Country'. This column will be derived from the existing location column and will only contain the 2-Alpha code of the country. By doing so, we can easily group and compare data at the country level, which will provide us with more meaningful insights for our analysis.\n",
    "\n",
    "further on we will use the `pycountry` module which is big list of countries in the ISO 3166-1, ISO 4217 formats.\n",
    "[Github Repository.](https://github.com/flyingcircusio/pycountry)\n",
    "\n",
    "\n",
    "by using the `pycountry.subdivisions.lookup` method and a quick solution we can narrow down the process of changing every US city and state cell to just the 2-Alpha code 'US'. this easen the burden much since most of our data comes from the US. this method left us with only the name of the cities for other countries.\n",
    "\n",
    "doing so we can rewrite the `get_country` function to now ignore the cells which have the 'US' value in them and use just the city names to figure out the 2-Alpha code of that country. for this matter the `geonamescache` [module](https://pypi.org/project/geonamescache/) will be used. this will only leave a small chunk of the data which can be manually edited to attain the finalized Data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "\n",
    "def get_country(location):\n",
    "    city, state = location.split(\", \")[:2] #leave only city and state name\n",
    "    try:\n",
    "        country = ''\n",
    "        if pycountry.subdivisions.lookup(f'US-{state}'):\n",
    "            country = \"US\"\n",
    "    except LookupError:\n",
    "            country = city\n",
    "    return country\n",
    "\n",
    "Frame = Data\n",
    "# Apply the get_country function to the location column to create a new column for country names\n",
    "Frame['Country'] = Frame['location'].apply(get_country)\n",
    "Frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import geonamescache\n",
    "\n",
    "# Load the geonamescache data\n",
    "gc = geonamescache.GeonamesCache()\n",
    "\n",
    "def get_country_code(location):\n",
    "    if location == \"US\":\n",
    "        return \"US\"\n",
    "    else:\n",
    "        try:\n",
    "            city = location\n",
    "            country_dict = gc.get_cities_by_name(city)[0]\n",
    "            second_part = list(country_dict.values())[0]\n",
    "            country_code = second_part['countrycode']\n",
    "        except (IndexError, KeyError):\n",
    "            country_code = location\n",
    "        return country_code\n",
    "    \n",
    "Geo_frame = Frame\n",
    "Geo_frame['Country'] = Geo_frame['Country'].apply(lambda x: get_country_code(x) if x != \"US\" else \"US\").drop(columns='location')\n",
    "Geo_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n",
    "\n",
    "## Finishing Touches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replace the 'Location' column with the column 'Country'\n",
    "country = Geo_frame['Country']\n",
    "Geo_frame['location'] = pd.DataFrame(country)\n",
    "\n",
    "# last touches\n",
    "Geo_frame.drop(columns=['Country'], inplace=True)\n",
    "Geo_frame.rename(columns={'location': 'Country'})\n",
    "Geo_frame.columns = [col.capitalize() for col in Geo_frame.columns]\n",
    "\n",
    "# loop through each column and fill null values with \"NA\"\n",
    "for col in Geo_frame.columns:\n",
    "    Geo_frame[col].fillna(value='NA', inplace=True)\n",
    "    \n",
    "Geo_frame.head()\n",
    "Geo_frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n",
    "\n",
    "## Outlier Detection\n",
    "\n",
    "Now that the data is clean and we have our desired columns we can run a outlier detection process for this matter the scipy module is used. Scipy is an open-source scientific computing library in Python that provides modules for scientific and technical computing.\n",
    "\n",
    "In the code below, we first compute the z-scores for all the numerical columns of the DataFrame df. The z-score is a measure of how many standard deviations a data point is away from the mean of its column. We use np.abs() to take the absolute values of the z-scores, since we're interested in detecting both positive and negative outliers.\n",
    "\n",
    "Next, we set a threshold above which a data point is considered an outlier. In this example, we chose a threshold of 3, which corresponds to a probability of less than 0.3% of a data point being that far away from the mean by chance, assuming a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# specify columns to check for outliers\n",
    "cols_to_check = ['Totalyearlycompensation', 'Basesalary', 'Stockgrantvalue', 'Bonus']\n",
    "\n",
    "# define function to remove outliers from specified columns\n",
    "def remove_outliers(Geo_frame, cols):\n",
    "    for col in cols:\n",
    "        z_scores = np.abs(stats.zscore(Geo_frame[col]))\n",
    "        threshold = 3\n",
    "        Geo_frame = Geo_frame[(z_scores < threshold) | (Geo_frame[col].isnull())]\n",
    "    return Geo_frame\n",
    "\n",
    "# remove outliers from specified columns\n",
    "Geo_frame = remove_outliers(Geo_frame, cols_to_check)\n",
    "\n",
    "Geo_frame.head()\n",
    "Geo_frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n",
    "\n",
    "## Outputting the Finalized Data?\n",
    "\n",
    "Now that we have a finalized data frame, we need to save it to a .csv for future use. To do this, we will use the `to_csv()` method in pandas, which allows us to save the data frame to a CSV file format. To ensure that the file is successfully saved, we will use the `os` library in Python to check that the file exists and has the expected file size. This step is important to ensure that the data is saved correctly and can be used in future analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH = 'data/jobs_with_country_codes_1.csv'\n",
    "Geo_frame.to_csv(PATH, index=False)\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(PATH):\n",
    "    # Get the file details\n",
    "    file_size = os.path.getsize(PATH)\n",
    "    file_created = os.path.getctime(PATH)\n",
    "    file_modified = os.path.getmtime(PATH)\n",
    "\n",
    "    print(f\"The file '{PATH}' exists.\")\n",
    "    print(f\"Size: {file_size} bytes\")\n",
    "    print(f\"Created: {file_created}\")\n",
    "    print(f\"Last modified: {file_modified}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"The file '{file_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data is Now Ready to be Visualized!\n",
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
